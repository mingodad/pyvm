/*
 *  General purpose segmented allocator
 * 
 *  Copyright (c) 2006 Stelios Xanthakis
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) version 3 of the License.
 */

/*
 * the libc's malloc is most interested in space efficiency at 'acceptable'
 * speed, and rightly so since malloc is a space function.
 * But malloc does take time and we'll lose in all benchmarks if we use
 * plain libc's malloc against custom allocators, since moreover pyvm will
 * be tested in programs with millions of small allocs due to its data model.
 *
 * this is the segmented allocator. Most important thing is that allocations
 * of the same size (8,16,24,..) are groupped together and this avoids the
 * fragmentation of memory caused in the simple malloc.  allocating/freeing
 * memory for such requests is done in one step. same for realloc.
 *
 * unlike python's obmalloc, we don't use arenas and try to return
 * segments to the OS if many are unused. Although, there are situations where
 * this allocator may actually hold more memory than py-obmalloc.
 *
 */
#include "include.h"
#include "mallok.h"
#include "inits.h"
extern int posix_memalign (void**, size_t, size_t);
#include "seg-malloc.h"
#define if_unlikely(...) if (__builtin_expect (__VA_ARGS__, 0))

/*
 * very useful macro to catch double-free
 * If things go really wrong, enable.
 * double-free means refcounting is broken
 */
//#define CHECK_DOUBLE_FREE

#define COLS "\033[01;37m"
#define COLE "\033[0m"

#define WORD *sizeof(long)

/*** tunable parameters ***/

/*** Block size 4k
     This is our only option because otherwise there is a possibility
     of segfault when we check whether some memory is malloced with
     this allocator or with normal malloc()
 ***/
#define BLOCKSIZE (4 * 1024)

/***
  The verification tables (see below) are fixed.
  There is one static table with 256 pointers that can point to
  tables of 256 pointers to segments (each segment == 4k)

  Thus there can be a maximum of 256*256 pages allocated (256MB)
  Minimum base memory always used = 2 * 256 * 4 = 2k

  Remember that this is about seg-mallocd memory for request <= 256 bytes!
 ***/
#define GLOBDISP 256

#define SEGMASK (~(BLOCKSIZE - 1))
#define NBLOCKS (BLOCKSIZE - 8 WORD) / sizeof (_block)

typedef unsigned long int segID;

static int n_segments;

int seg_n_segments ()
{
	return n_segments;
}

class segment
{
typedef	_block = 0;
	segID ID;
	/* A segment can be in the "usable" list if it has free slots.
	   Alternatively, it can be in the "freeseg" slot if it is completely
	   empty, or in the "full" list if it's completely full and SEGSTATS
	   is enabled, or in no list otherwise.  */
	segment *next, *prev;
	long used, max, fill, bs;
	void *ffree;
auto	_block chunk [NBLOCKS];

	void init ();
auto	segment ();
	~segment ();
};

static inline int size_to_class (int s)
{
	return s < 128 ? s >> 3 : (s >> 4) + 8;
}

/* --------------* lookup verifiers *-------------- */

/*
 * In order to check if an address is allocated
 * with a segment, we use double-check verification.
 * Each segment has a pointer to its "Verify" index
 * and each "Verify" entry points to a segment.
 * If these two agree we are in a segment alright, but
 * if they don't, we may have accessed uninitialized memory.
 */

union segptr {
	segment *S;
	segID next_free;
};

static union segptr Tbl0 [GLOBDISP];
static union segptr *Verify [GLOBDISP] = { Tbl0, };
static segID avail, max_ID;
static unsigned int max_ver;

static inline union segptr *verifier (int i)
{
	return &Verify [i / GLOBDISP][i % GLOBDISP];
}

static void init_verifier (int t)
{
#ifdef	CHECK_DOUBLE_FREE
fprintf (stderr, "initverifier (%i)\n", t);
#endif
	int i;
	union segptr *Tbl = Verify [t];
	t *= GLOBDISP;
	for (i = 0; i < GLOBDISP - 1; i++)
		Tbl [i].next_free = t + i + 1;
	Tbl [i].next_free = -1;
	avail = t;
	max_ID = t + i;
}

static class InitMem : InitObj {
	int priority = INIT_MEMORY;
	void todo ()
	{
		init_verifier (0);
	}
};

static void extend_verifiers ()
{
	Verify [++max_ver] = (union segptr*) __malloc (sizeof Tbl0);
	init_verifier (max_ver);
}

static inline segment *__segmentof (void *p)
{
	return (segment*) ((long) p & SEGMASK);
}

static segment *__in_segment (void *p)
{
	/* may cause uninitialized memory access
	 */
	segment *s = (segment*) ((long) p & SEGMASK);
	segID ID = s->ID;
	return ID <= max_ID && verifier (ID)->S == s ? s : 0;
}

/* ------------------------------------------------ */

static inline segment *alloc_segment ()
{
	/* currently, pages are acquired through dl-malloc, memalign.
	 * it's supposed to be pretty fast for 4kb requests. We may
	 * write a custom page allocator in the future...
	 */
	void *ptr;
	if_unlikely (!(ptr = __memalign (BLOCKSIZE, BLOCKSIZE))) {
		/* In modern OSs, malloc never fails.
		 * Instead the process is killed if there's no memory.
		 * So do the same here instead of throwing'n stuff.
		 * Still, we can raise some interrupt but the malloc path
		 * should be considered non-throwing (and local REFPTRs
		 * not protected for unwind). We'd like some exception
		 * that cannot be caught in order to do this though.
		 */
		fprintf (stderr, "OUT OF MEMORY:(");
		exit (1);
	}
	return ptr;
}

void segment.init ()
{
	if (avail == -1)
		extend_verifiers ();
	ID = postfix (avail, avail = verifier (avail)->next_free);
	verifier (ID)->S = this;
	fill = used = 0;
	prev = next = 0;
}

segment.segment ()
{
	++n_segments;
	max = NBLOCKS;
	ffree = &chunk [0];
	chunk [0].next = 0;
	bs = size_to_class (sizeof (_block) - 1);
}

segment.~segment ()
{
	--n_segments;
	verifier (ID)->next_free = avail;
	avail = ID;
}

#define DEFSEG(X) \
	struct chunk ## X { union {\
		char block [X];\
		void *next;\
		}; };\
	static inline class segment ## X : segment {\
	typedef chunk ## X _block;\
	};

DEFSEG(8)   DEFSEG(16)  DEFSEG(24)  DEFSEG(32)
DEFSEG(40)  DEFSEG(48)  DEFSEG(56)  DEFSEG(64)
DEFSEG(72)  DEFSEG(80)  DEFSEG(88)  DEFSEG(96)
DEFSEG(104) DEFSEG(112) DEFSEG(120) DEFSEG(128)
DEFSEG(144) DEFSEG(160) DEFSEG(176) DEFSEG(192)
DEFSEG(208) DEFSEG(224) DEFSEG(240) DEFSEG(256)

#define MAX_SEG_ALLOC 256

static inline void *segment8.offsetof_last_unfilled (int sz)
{
	return ((void*) chunk) + sz * fill;
}

static segment *freeseg [24];
static segment *usable [24];
static const int sizes [] = {
	8,16,24,32,40,48,56,64,72,80,88,96,104,112,120,128,144,160,176,192,208,224,240,256
};

//#define SEGSTATS
#ifdef SEGSTATS
static segment *full [24];
void seg_stats ()
{
	int arr [24];
	memset (arr, 0, sizeof arr);
	int i;
	segment *s;

	for (i = 0; i < 24; i++) {
		for (s = usable [i]; s; s = s->next) arr [i]++;
		if (freeseg [i]) arr [i]++;
		for (s = full [i]; s; s = s->next) arr [i]++;
	}
	printf ("[");
	for (i =0;i<24;i++)
		printf ("%i,", arr [i]);
	printf ("] %i\n", n_segments);
}
#endif

void segment.mvto_usable (int n)
{
#ifdef SEGSTATS
	if (freeseg [n] != this) {
		if (next) next->prev = prev;
		if (prev) prev->next = next;
		else full [n] = next;
	}
#endif
	prev = 0;
	if ((next = usable [n]))
		next->prev = this;
	usable [n] = this;
}

void segment.rmfrom_usable (int n)
{
	if (next) next->prev = prev;
	if (prev) prev->next = next;
	else usable [n] = next;
}

void segment.rmfirstfrom_usable (int n)
{
	if ((usable [n] = next)) next->prev = 0;
#ifdef SEGSTATS
	prev = 0;
	if ((next = full [n]))
		next->prev = this;
	full [n] = this;
#endif
}

template fixed_alloc(N) {
	/*** inline seg_alloc for very common known sizes ***/
	void *seg_alloc >< N ()
	{
		segment *s;
		if ((s = usable [size_to_class (N - 1)]) && *(void**) s->ffree) {
			++s->used;
			return postfix (s->ffree, s->ffree = *(void**) s->ffree);
		}
		return seg_alloc (N);
	}
}

fixed_alloc (96)
fixed_alloc (56)
fixed_alloc (32)
fixed_alloc (24)

void *FASTCALL1 seg_alloc (size_t c)
{
	void *ret;
	segment *s;

	if (c > MAX_SEG_ALLOC) return __malloc (c);

	unsigned int n = size_to_class (c - 1);

	s = usable [n];
	if (s) have_usable: {
		++s->used;
		ret = s->ffree;
		if (*(void**) ret) {
			s->ffree = *(void**) ret;
			return ret;
		}
		if (++s->fill < s->max) {
			s->ffree = ((segment8*) s)->offsetof_last_unfilled (sizes [n]);
			*(void**) s->ffree = 0;
			return ret;
		}
		s->ffree = 0;
		s->rmfirstfrom_usable (n);
		return ret;
	}

	if ((s = freeseg [n])) {
		s->mvto_usable (n);
		freeseg [n] = 0;
		goto have_usable;
	}

	s = alloc_segment ();
	s->init ();
	usable [n] = s;
	switch (n) {
#define NEWSEG(X,Y) case X: ((segment ## Y*)s)->ctor (); goto have_usable;
	NEWSEG(0,8)    NEWSEG(1,16)   NEWSEG(2,24)   NEWSEG(3,32)   NEWSEG(4,40) NEWSEG(5,48)
	NEWSEG(6, 56)  NEWSEG(7, 64)  NEWSEG(8,72)   NEWSEG(9,80)   NEWSEG(10,88) NEWSEG(11,96)
	NEWSEG(12,104) NEWSEG(13,112) NEWSEG(14,120) NEWSEG(15,128)
	NEWSEG(16,144) NEWSEG(17,160) NEWSEG(18,176)
	NEWSEG(19,192) NEWSEG(20,208) NEWSEG(21,224) NEWSEG(22,240) NEWSEG(23,256) 
	}

	return 0;
}

/* free something which is definitelly smaller than 256 bytes
   and consequently definitelly in a segment  */
void FASTCALL1 seg_freeXX (void *p)
{
	segment *s = __segmentof (p);
#ifdef	CHECK_DOUBLE_FREE
	{
		void *xx = s->ffree;
		while (xx) {
			if (xx == p) {
				fprintf (stderr, "Double free\n");
				*(int*)0 = 0;
			}
			xx = *(void**)xx;
		}
	}
#endif

	if (s->used < s->max && s->used > 1) {
		--s->used;
		*(void**)p = s->ffree;
		s->ffree = p;
		return;
	}
	seg_free_segmented (s, p);
}


static void seg_free_segmented (segment *s, void *p)
{
#ifdef	CHECK_DOUBLE_FREE
	{
		void *xx = s->ffree;
		while (xx) {
			if (xx == p) {
				fprintf (stderr, "Double free\n");
				*(int*)0 = 0;
			}
			xx = *(void**)xx;
		}
	}
#endif

	*(void**)p = s->ffree;
	s->ffree = p;
	if (s->used-- < s->max) {
		if (s->used)
			return;
		if (!s->next && !s->prev)
			return;
		if (!freeseg [s->bs]) {
			s->rmfrom_usable (s->bs);
			freeseg [s->bs] = s;
			return;
		}
		s->rmfrom_usable (s->bs);
		delete s;
		return;
	}
	s->mvto_usable (s->bs);
}

void FASTCALL1 seg_free (void *p)
{
	segment *s;

	if ((s = __in_segment (p)))
		seg_free_segmented (s, p);
	else	__free (p);
}

/* The client is not required to store the size of the allocated quantity
   but if it knows it this is a useful hint for the allocator.  */
void FASTCALL2 seg_free (void *p, size_t s)
{
	if (s <= MAX_SEG_ALLOC) seg_freeXX (p);
	else __free (p);
}

#if 1
// word aligned, helps builtin_memcpy
static inline void memcpy_44 (unsigned int *a, unsigned int *b, int n)
{
	memcpy (a, b, n);
}
#else
#define memcpy_44 memcpy
#endif

void *seg_realloc (void *p, size_t c)
{
	segment *s;
	void *r;

	if ((s = __in_segment (p))) {
		if (c <= MAX_SEG_ALLOC) {
			unsigned int n = size_to_class (c - 1);
			if (n == s->bs) return p;
			memcpy_44 (r = seg_alloc (c), p, c);
			seg_free_segmented (s, p);
			return r;
		}
		memcpy (r = __malloc (c), p, sizes [s->bs]);
		seg_free_segmented (s, p);
		return r;
	}

	if (c <= MAX_SEG_ALLOC) {
		memcpy (r = seg_alloc (c), p, c);
		__free (p);
		return r;
	}
	return __realloc (p, c);
}

void *seg_realloc2 (void *p, size_t o, size_t c)
{
	segment *s;
	void *r;

	if (o <= MAX_SEG_ALLOC) {
		s = __segmentof (p);
		if (c <= MAX_SEG_ALLOC) {
			unsigned int n = size_to_class (c - 1);
			if (n == s->bs) return p;
			memcpy_44 (r = seg_alloc (c), p, c);
			seg_free_segmented (s, p);
			return r;
		}
		memcpy (r = __malloc (c), p, sizes [s->bs]);
		seg_free_segmented (s, p);
		return r;
	}

	if (c <= MAX_SEG_ALLOC) {
		memcpy (r = seg_alloc (c), p, c);
		__free (p);
		return r;
	}
	return __realloc (p, c);
}
// do you remeber malloc?
