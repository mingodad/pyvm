##  Loading PDF
##
##  This program is free software; you can redistribute it and/or modify
##  it under the terms of the GNU General Public License as published by
##  the Free Software Foundation; either version 2 of the License, or
##  (at your option) version 3 of the License.

__autosem__

from misc import interner
from stream import stream
from base import Cpdf, dictFromList

# We read a PDF file and return the "trailer" object.
# This object points to other objects, including the "Root"
# catalog which points to pages, etc...

#
# Regular expressions for tokenization
#

namespace Re
{
	from re import compile
	isNumber  = compile (r"[+-]?(?:\d+\.?\d*|\.\d+)").match
	String    = compile (r"[^%/(){}[\]\s<>]*").match
	Special   = compile (r"<<|>>|]|\[|\w+").match
	Comment   = compile (r"%[^\r\n]*[\r\n]").match
	Command   = compile (r"[^/%(){}[\]\s<>]+").match
	xrefs     = compile (r"\d+\s+(\d+)\s*?(?:\r\n|\r|\n)").match
	NL        = compile (r"\r\n|\n|\r").search
	EOF       = compile (r"\s*startxref[\r\n\s]+\d+[\r\n\s]+%%EOF[\r\n]*").match
	del compile
}

#
# objects
#

class Marker method __init__ ($pos);

DSpecial = {
	"true":True,
	"false":False,
	"null":None,
}

class SpecialTok
{
	method __init__ ($t, $flag=0) DSpecial [t] = self
	method __str__ () return '<special "%s">'%$t
}

namespace NSpecial
{
	OpenDict   = SpecialTok ("<<", 2)
	CloseDict  = SpecialTok (">>", 1)
	OpenArray  = SpecialTok ("[", 2)
	CloseArray = SpecialTok ("]", 1)
	Refer      = SpecialTok ("R", 1)
	OpenObj    = SpecialTok ("obj")
	CloseObj   = SpecialTok ("endobj")
}

class Error
{
	method __init__ ($pos) if (pos -> int) $pos = "Error at offset %i" %pos
	method __str__ () return $pos
}

#
# Lexers
#

def get_string (txt, pos)
{
	b = _buffer (Cpdf.scan_string (txt, pos))
	return b, Cpdf.get_string (txt, pos, b)
}

gen gentokens (txt, pos=0)
{
	spdict   = {
		4: DSpecial ["["],
		5: DSpecial ["]"],
		6: DSpecial ["<<"],
		7: DSpecial [">>"],
	}
	dspecial = DSpecial

	Special  = Re.Special
	Comment  = Re.Comment
	keyname  = interner ()

	pdftoks = _buffer (12)
	gettok = Cpdf.gettok
	setpos = Cpdf.setpos
	rv = array ("i", 2)
	Cpdf.initok (pdftoks, txt, len (txt), rv, pos)

	while ((pos = gettok (pdftoks)) != -1) {
		if.continue (r0 = rv [0])
			if (r0 < 4) {
				if (r0 == 1)		# integer
					yield rv [1]
				else if (r0 == 2)	# float
					yield float (txt [rv [1]:pos])
				else			# names (strings)
					yield keyname (txt [rv [1]:pos])
			} else if (r0 == 8) {		# true, false, null, R, obj, endobj, stream, xref
				try yield dspecial [txt [rv [1]:pos]]
				except {
					k = txt [rv [1]:pos]
					if (k == "stream") {
						if (txt [pos] == "\n") pos += 1
						else pos += 2
						p2 = txt.index ("endstream", pos)
						yield stream (txt [pos:p2])
						pos = p2 + 9
						setpos (pdftoks, pos)
					} else if (k in ("xref", "startxref")) {
						raise Marker (pos - len (k))
					} else raise Error (pos)
				}
			} else yield spdict [r0]	# [ ] << >>

		# strings
		if.continue (txt [pos] == '(') {
			k, pos = get_string (txt, pos + 1)
			yield k
			setpos (pdftoks, pos)
		}

		if.continue (txt [pos] == "<") {
			pos += 1
			p2 = txt.index ('>', pos)
			# (dewhitespace)
			yield unhexlify (txt [pos:p2])
			pos = p2 + 1
			setpos (pdftoks, pos)
		}

		if (R = Comment (txt, pos)) {
			setpos (pdftoks, R.end ())
		} else {
			print "CRAPS!: [%r]"% txt [pos:pos+4]
			raise Error (pos)
		}
	}
}

#
# Object parser
#

gen parse_objects (tokenizer, Refs)
{
	# parse and yield objects from the tokens
	# generated by the tokenizer. The tokenizer
	# will raise a "Marker" exception when it
	# encounters a keyword like "xref" or "startxref"
	# that is not an object definition.

	CloseObj   = NSpecial.CloseObj
	Refer      = NSpecial.Refer
	CloseDict  = NSpecial.CloseDict
	CloseArray = NSpecial.CloseArray
	OpenArray  = NSpecial.OpenArray
	Next       = tokenizer.next

	O = []
	I = []
	Ipop = I.pop

	while (1) {
		i0 = Next ()
		i1 = Next ()
		Next ()		# must be "obj"
		for (N in tokenizer) {
			if (N => SpecialTok) {
				if.break (N is CloseObj)
					yield i0 + (i1 << 17), O.pop ()

				if (N.flag == 2) I.append (len (O))
				else if (N is Refer) {
					# object references are stored as tuples
					# until we transreference the document.
					id, ge = O [-2:]
					id += ge << 17
					try r = Refs [id]
					except r = Refs [id] = (id,)
					O [-2:] = [r]
				} else if (N is CloseDict) {
					x = Ipop ()
					d = dictFromList (O [x:])
					## Hack. The special dictionary "Link" may exist
					## 100ths of thousands of times. Instead of a dict,
					## use a list and save tons of memory
					if (d.get ("Subtype") == "Link")
						d = O [x:]
					O [x:] = [d]
				} else {
					x = Ipop ()
					O [x:] = [O [x:]]
				}
			} else if (N => stream) {
				N.set_dict (O [-1])
				O [-1] = N
				if (N.Type == "ObjStm")
					for (idobj in parse_object_stream (N.decode (), N.d ["N"], Refs))
						yield idobj
			} else O.append (N)
		}
	}
}

gen parse_object_stream (txt, N, Refs)
{
	# An object stream (new in pdf 1.5) is a stream of objects. But objects/ids
	# are arranged with a completely different format.  Duplicate the parser.

	CloseDict  = NSpecial.CloseDict
	Refer      = NSpecial.Refer
	t = gentokens (txt, 0)
	ids = []
	for (None in *N) {
		ids.append (t.next ())
		t.next ()
	}

	O = [];
	I = [];
	try for (o in t)
		if (o => SpecialTok) {
			if (o.flag == 2) I.append (len (O));
			else if (o is Refer) {
				id, ge = O [-2:];
				id += ge << 17;
				try r = Refs [id];
				except r = Refs [id] = (id,);
				O [-2:] = [r];
			} else if (o is CloseDict) {
				x = I.pop ();
				d = dictFromList (O [x:]);
				## Kludge.  PDF1.6.pdf contains "Named Destinations".
				## While the entire document has 45000 dictionaries
				## there are another 65000 dictionaries which are
				## named destinations!! Not only we do not use these
				## at the moment, but a different data structure
				## would be preferred.  For now, we NULLify all dicts
				## that have only one key 'D', assuming that only
				## named destinations have such dicts.  Thus speed up
				## and save lots of memory.
				if (len (d) == 1 and "D" in d)
					d = None;
				O [x:] = [d];
			} else {
				x = I.pop ();
				O [x:] = [O [x:]];
			}
		} else O.append (o);

	for (i in zip (ids, O))
		yield i;
}

def parse_trailer (tokenizer, Refs)
{
	Refer      = NSpecial.Refer;
	CloseDict  = NSpecial.CloseDict;
	I = [0];
	O = [];
	try
	for (N in tokenizer) {
		if (N => SpecialTok) {
			if (N.flag == 2) I.append (len (O));
			else if (N is Refer) {
				id, ge = O [-2:];
				id += ge << 17;
				try r = Refs [id];
				except r = Refs [id] = (id,);
				O [-2:] = [r];
			} else if (N is CloseDict) {
				x = I.pop ();
				O [x:] = [dictFromList (O [x:])];
			} else {
				x = I.pop ();
				O [x:] = [O [x:]];
			}
		} else if (N => stream) {
			N.set_dict (O [-1]);
			O [-1] = N;
		} else
			O.append (N);
	}
	except (Marker, m) return O [0], m.pos;
}

def skip_xref (txt, pos)
{
	# Skip the "xref" section -- we won't use it.
	# The xref section is supposed to mark the offset
	# of objects in the file, so that theoretically
	# the program can read an object on-demand.
	# But we are reading all the objects anyway.

	Xrefs = Re.xrefs;
	NL    = Re.NL;
	while (txt [pos] in "\r\n") pos += 1;

	while (1) {
		try n = int (Xrefs (txt, pos) [1]);
		except break;
		for (n in *(n + 1))	# (times, repeat)
			pos = NL (txt, pos).end ();
	}

	return pos;
}

def read_pdf (txt)
{
	Objects = {};
	Refs = {};
	trailer = {};

	pos = 0;
	while (1) {
		# objects
		try {
			for (id, obj in parse_objects (gentokens (txt, pos), Refs)) 
				Objects [id] = obj;
			raise Error (-1);
		} except (Marker, m) pos = m.pos;

		# xref + trailer
		if (txt [pos:pos+4] == "xref") {
			pos = skip_xref (txt, pos + 4);
			if (txt [pos:pos+7] == "trailer") {
				t, pos = parse_trailer (gentokens (txt, pos + 7), Refs);
				t ["NextN"] = trailer;
				if (trailer) t.update (trailer);
				trailer = t;
			}
		}

		# startxref \d %%EOF
		pos = Re.EOF (txt, pos).end ();

		# incremental updates?
		try txt [pos];
		except break;
	}

	if (!trailer) {
		# xref streams. ignore the stream part.
		for (k in Objects.itervalues ())
			if (k => stream and k.Type == "XRef")
				trailer.update (k.d);
		if (!trailer)
			raise "Error: no trailer found";
	}

	Objects ["trailer"] = trailer;
	return Objects;
}

def TransReference (objs)
{
	# Replace references with the real object
	# (because python works with references anyway).

	IGN = int.__sigbit__ | float.__sigbit__ | str.__sigbit__
		 | type (None).__sigbit__ | bool.__sigbit__;
	def transref (x)
	{
		if (x -> list) itr = x;
		else if (x -> dict) itr = x.itervalues ();
		else itr = x.d.itervalues ();	# stream

		for (i in itr)
			if (i -> tuple)
				__SET_ITER__ (objs [i [0]]);
			else if (not (IGN & __sigbit__ (i)))
				transref (i);
	}

	for (v in objs.itervalues ())
		if (not (IGN & __sigbit__ (v)))
			transref (v);
}

#
# main
#

def load0 (filename)
{
	from elements import Document
	objs = read_pdf (open (filename).read ())
	TransReference (objs)
#for (v in objs.itervalues ())
#if (v->dict) print v.keys ();
#from apps import objview;
#objview.objnav (objs ["trailer"]);
	return Document (objs ["trailer"])
}

def load (filename)
{
	# Report some statistics (suppress)
	print "Loading document..."
	sys.gc.collect ()
	n = modules.vmstats.count_dicts ()
	Doc = load0 (filename)
	sys.gc.collect ()
	n2 = modules.vmstats.count_dicts ()
	print "(%i dictionaries)" %(n2-n)
	return Doc
}

if (__name__ == __main__)
	with (@profile.Profile ())
		load (sys.argp.filename ())
